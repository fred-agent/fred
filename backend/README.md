# Fred Agentic Backend

Fred is a flexible agentic backend that makes it easy to build, compose, and operate expert agents â€” including multi-agent systems with a planning **leader** agent.

It provides:

- âš™ï¸ A powerful runtime to orchestrate **domain-specific experts** and **leaders**
- ğŸ§  Built-in support for OpenAI, Azure, Ollama, and other LLM providers
- ğŸ§ª Local development defaults for quick experimentation
- ğŸ”Œ Optional integration with external backends (MCP servers, OpenSearch, MinIO, etc.)

---

## ğŸš€ Getting Started

You can spin up the backend in seconds:

```bash
make run
```

This will:
- Start the API server (FastAPI)
- Use default in-memory and local storage components
- Let you interact with agents right away â€” no external dependencies required

---

## Configure Your LLM Provider (Required)

To use Fred, you must configure an LLM provider in `configuration.yaml` and `.env`.

Fred supports:

- âœ… **OpenAI**
- âœ… **Azure OpenAI**
- âœ… **Azure via API Management (APIM)**
- âœ… **Ollama** (for local models like `llama2`, `mistral`, etc.)

See [LLM Configuration](#-configuring-freds-ai-model-provider-openai-azure-apim) below for details.

---

## Optional External Backends

Fred is modular â€” it can integrate with:

- ğŸŸ¤ **MCP servers** (for code execution, monitoring, document search, etc.)
- ğŸ” **OpenSearch** (for persistent vector storage)
- ğŸª£ **MinIO** (for storing feedback, files, or context)
- â˜ï¸ **Cloud storage** (via the `context_storage` and `dao` configs)

These are optional. By default, Fred uses local file-based context cache.

You can plug in real backends incrementally, agent by agent.

---

## Configuring Fred's AI Model Provider (OpenAI, Azure, APIM)

Fred supports multiple AI model providers through a flexible YAML configuration and environment-based secret management.

You can choose between:

- âœ… **OpenAI** (e.g., `gpt-4o`, `gpt-3.5-turbo`)
- âœ… **Azure OpenAI** (with your own Azure deployment)
- âœ… **Azure via API Management (APIM)** (for enterprise environments using gateways)

### ğŸ“ Step 1: Edit `configuration.yaml`

Inside the `ai.default_model` section, choose your provider.

#### ğŸ”¹ Option 1: OpenAI
```yaml
ai:
  default_model:
    provider: "openai"
    name: "gpt-4o"
    provider_settings:
      temperature: 0.0
      max_retries: 2
      request_timeout: 30
```

#### ğŸ”¹ Option 2: Azure OpenAI
```yaml
ai:
  default_model:
    provider: "azure"
    name: "fred-gpt-4o"  # your Azure deployment name
    provider_settings:
      api_version: "2024-05-01-preview"
      temperature: 0.0
      request_timeout: 30
      max_retries: 2
```

#### ğŸ”¹ Option 3: Azure OpenAI via APIM
```yaml
ai:
  default_model:
    provider: "azure"
    name: "fred-gpt-4o"
    provider_settings:
      api_version: "2024-05-01-preview"
      temperature: 0.0
      request_timeout: 30
      max_retries: 2
      azure_endpoint: "https://your-company-api.azure-api.net"
```

### ğŸ” Step 2: Set Required Environment Variables in `.env`

#### âœ… For OpenAI:
```env
OPENAI_API_KEY=sk-...
```

#### âœ… For Azure OpenAI:
```env
AZURE_OPENAI_API_KEY=...
```

Optional for Azure AD authentication:
```env
AZURE_TENANT_ID=...
AZURE_CLIENT_ID=...
AZURE_CLIENT_SECRET=...
```

#### âœ… For Azure APIM (if used):
```env
AZURE_APIM_KEY=your-subscription-key
```

---

## Developer Tips

- Agents can be enabled/disabled in `configuration.yaml` under `ai.agents`
- Each agent can specify its own model and MCP server config
- The leader agent coordinates other experts when enabled
- Logs are verbose by default to help with debugging

---

## Local Setup Only?

Yes â€” everything works out of the box on a developer laptop. You can later plug in production storage or APIs.

---

## License

Licensed under the [Apache License 2.0](LICENSE).
